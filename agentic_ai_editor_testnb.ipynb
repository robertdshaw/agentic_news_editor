{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75ce587e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>newsID</th>\n",
       "      <th>category</th>\n",
       "      <th>subcategory</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>url</th>\n",
       "      <th>title_entities</th>\n",
       "      <th>abstract_entities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>N88753</td>\n",
       "      <td>lifestyle</td>\n",
       "      <td>lifestyleroyals</td>\n",
       "      <td>The Brands Queen Elizabeth, Prince Charles, an...</td>\n",
       "      <td>Shop the notebooks, jackets, and more that the...</td>\n",
       "      <td>https://assets.msn.com/labs/mind/AAGH0ET.html</td>\n",
       "      <td>[{\"Label\": \"Prince Philip, Duke of Edinburgh\",...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>N45436</td>\n",
       "      <td>news</td>\n",
       "      <td>newsscienceandtechnology</td>\n",
       "      <td>Walmart Slashes Prices on Last-Generation iPads</td>\n",
       "      <td>Apple's new iPad releases bring big deals on l...</td>\n",
       "      <td>https://assets.msn.com/labs/mind/AABmf2I.html</td>\n",
       "      <td>[{\"Label\": \"IPad\", \"Type\": \"J\", \"WikidataId\": ...</td>\n",
       "      <td>[{\"Label\": \"IPad\", \"Type\": \"J\", \"WikidataId\": ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>N23144</td>\n",
       "      <td>health</td>\n",
       "      <td>weightloss</td>\n",
       "      <td>50 Worst Habits For Belly Fat</td>\n",
       "      <td>These seemingly harmless habits are holding yo...</td>\n",
       "      <td>https://assets.msn.com/labs/mind/AAB19MK.html</td>\n",
       "      <td>[{\"Label\": \"Adipose tissue\", \"Type\": \"C\", \"Wik...</td>\n",
       "      <td>[{\"Label\": \"Adipose tissue\", \"Type\": \"C\", \"Wik...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>N86255</td>\n",
       "      <td>health</td>\n",
       "      <td>medical</td>\n",
       "      <td>Dispose of unwanted prescription drugs during ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://assets.msn.com/labs/mind/AAISxPN.html</td>\n",
       "      <td>[{\"Label\": \"Drug Enforcement Administration\", ...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>N93187</td>\n",
       "      <td>news</td>\n",
       "      <td>newsworld</td>\n",
       "      <td>The Cost of Trump's Aid Freeze in the Trenches...</td>\n",
       "      <td>Lt. Ivan Molchanets peeked over a parapet of s...</td>\n",
       "      <td>https://assets.msn.com/labs/mind/AAJgNsz.html</td>\n",
       "      <td>[]</td>\n",
       "      <td>[{\"Label\": \"Ukraine\", \"Type\": \"G\", \"WikidataId...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   newsID   category               subcategory  \\\n",
       "0  N88753  lifestyle           lifestyleroyals   \n",
       "1  N45436       news  newsscienceandtechnology   \n",
       "2  N23144     health                weightloss   \n",
       "3  N86255     health                   medical   \n",
       "4  N93187       news                 newsworld   \n",
       "\n",
       "                                               title  \\\n",
       "0  The Brands Queen Elizabeth, Prince Charles, an...   \n",
       "1    Walmart Slashes Prices on Last-Generation iPads   \n",
       "2                      50 Worst Habits For Belly Fat   \n",
       "3  Dispose of unwanted prescription drugs during ...   \n",
       "4  The Cost of Trump's Aid Freeze in the Trenches...   \n",
       "\n",
       "                                            abstract  \\\n",
       "0  Shop the notebooks, jackets, and more that the...   \n",
       "1  Apple's new iPad releases bring big deals on l...   \n",
       "2  These seemingly harmless habits are holding yo...   \n",
       "3                                                NaN   \n",
       "4  Lt. Ivan Molchanets peeked over a parapet of s...   \n",
       "\n",
       "                                             url  \\\n",
       "0  https://assets.msn.com/labs/mind/AAGH0ET.html   \n",
       "1  https://assets.msn.com/labs/mind/AABmf2I.html   \n",
       "2  https://assets.msn.com/labs/mind/AAB19MK.html   \n",
       "3  https://assets.msn.com/labs/mind/AAISxPN.html   \n",
       "4  https://assets.msn.com/labs/mind/AAJgNsz.html   \n",
       "\n",
       "                                      title_entities  \\\n",
       "0  [{\"Label\": \"Prince Philip, Duke of Edinburgh\",...   \n",
       "1  [{\"Label\": \"IPad\", \"Type\": \"J\", \"WikidataId\": ...   \n",
       "2  [{\"Label\": \"Adipose tissue\", \"Type\": \"C\", \"Wik...   \n",
       "3  [{\"Label\": \"Drug Enforcement Administration\", ...   \n",
       "4                                                 []   \n",
       "\n",
       "                                   abstract_entities  \n",
       "0                                                 []  \n",
       "1  [{\"Label\": \"IPad\", \"Type\": \"J\", \"WikidataId\": ...  \n",
       "2  [{\"Label\": \"Adipose tissue\", \"Type\": \"C\", \"Wik...  \n",
       "3                                                 []  \n",
       "4  [{\"Label\": \"Ukraine\", \"Type\": \"G\", \"WikidataId...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "cols = [\"newsID\", \"category\", \"subcategory\", \"title\", \"abstract\", \"url\", \"title_entities\", \"abstract_entities\"]\n",
    "\n",
    "news_df = pd.read_csv(r\"C:\\Users\\rshaw\\Desktop\\EC Utbildning - Data Science\\Thesis\\Agentic_AI_News_Editor project\\agentic_ai_editor_project\\train_data\\news.tsv\", sep=\"\\t\", header=None, names=cols)\n",
    "\n",
    "# Preview first few rows\n",
    "display(news_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "029c679c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "newsID                  0\n",
      "category                0\n",
      "subcategory             0\n",
      "title                   0\n",
      "abstract             5415\n",
      "url                     0\n",
      "title_entities          3\n",
      "abstract_entities       6\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(news_df.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aaac085c",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df = news_df.dropna(subset=[\"abstract\", \"title_entities\", \"abstract_entities\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f6da5c0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "newsID               0\n",
      "category             0\n",
      "subcategory          0\n",
      "title                0\n",
      "abstract             0\n",
      "url                  0\n",
      "title_entities       0\n",
      "abstract_entities    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(news_df.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b62c43c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       title_length  abstract_length\n",
      "count  96106.000000     96106.000000\n",
      "mean      10.687512        38.282594\n",
      "std        3.243870        26.192824\n",
      "min        1.000000         1.000000\n",
      "25%        9.000000        17.000000\n",
      "50%       10.000000        27.000000\n",
      "75%       13.000000        67.000000\n",
      "max       48.000000       474.000000\n"
     ]
    }
   ],
   "source": [
    "# How long are the titles and abstracts?\n",
    "news_df[\"title_length\"] = news_df[\"title\"].apply(lambda x: len(x.split()))\n",
    "news_df[\"abstract_length\"] = news_df[\"abstract\"].apply(lambda x: len(x.split()))\n",
    "\n",
    "print(news_df[[\"title_length\", \"abstract_length\"]].describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4d6dfeb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>percentage</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>category</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>sports</th>\n",
       "      <td>29625</td>\n",
       "      <td>30.8%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>news</th>\n",
       "      <td>29363</td>\n",
       "      <td>30.6%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>finance</th>\n",
       "      <td>5777</td>\n",
       "      <td>6.0%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>travel</th>\n",
       "      <td>4605</td>\n",
       "      <td>4.8%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>video</th>\n",
       "      <td>4562</td>\n",
       "      <td>4.7%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>foodanddrink</th>\n",
       "      <td>4319</td>\n",
       "      <td>4.5%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lifestyle</th>\n",
       "      <td>4255</td>\n",
       "      <td>4.4%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weather</th>\n",
       "      <td>3820</td>\n",
       "      <td>4.0%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>health</th>\n",
       "      <td>2815</td>\n",
       "      <td>2.9%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>autos</th>\n",
       "      <td>2756</td>\n",
       "      <td>2.9%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tv</th>\n",
       "      <td>1291</td>\n",
       "      <td>1.3%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>music</th>\n",
       "      <td>1220</td>\n",
       "      <td>1.3%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>movies</th>\n",
       "      <td>803</td>\n",
       "      <td>0.8%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>entertainment</th>\n",
       "      <td>795</td>\n",
       "      <td>0.8%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kids</th>\n",
       "      <td>96</td>\n",
       "      <td>0.1%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>middleeast</th>\n",
       "      <td>2</td>\n",
       "      <td>0.0%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>games</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>northamerica</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               count percentage\n",
       "category                       \n",
       "sports         29625      30.8%\n",
       "news           29363      30.6%\n",
       "finance         5777       6.0%\n",
       "travel          4605       4.8%\n",
       "video           4562       4.7%\n",
       "foodanddrink    4319       4.5%\n",
       "lifestyle       4255       4.4%\n",
       "weather         3820       4.0%\n",
       "health          2815       2.9%\n",
       "autos           2756       2.9%\n",
       "tv              1291       1.3%\n",
       "music           1220       1.3%\n",
       "movies           803       0.8%\n",
       "entertainment    795       0.8%\n",
       "kids              96       0.1%\n",
       "middleeast         2       0.0%\n",
       "games              1       0.0%\n",
       "northamerica       1       0.0%"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "counts = news_df[\"category\"].value_counts()\n",
    "percs = news_df[\"category\"].value_counts(normalize=True).mul(100).round(1).astype(str) + '%'\n",
    "display(pd.concat([counts,percs], axis=1, keys=['count', 'percentage']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4f20a987",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "has_title_entities       0.743627\n",
       "has_abstract_entities    0.791220\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "news_df[\"has_title_entities\"] = news_df[\"title_entities\"].apply(lambda x: x != \"[]\")\n",
    "news_df[\"has_abstract_entities\"] = news_df[\"abstract_entities\"].apply(lambda x: x != \"[]\")\n",
    "\n",
    "display(news_df[[\"has_title_entities\", \"has_abstract_entities\"]].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "66e8bcdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[{\"Label\": \"Tesla Autopilot\", \"Type\": \"U\", \"WikidataId\": \"Q27150149\", \"Confidence\": 1.0, \"OccurrenceOffsets\": [0], \"SurfaceForms\": [\"Tesla Autopilot\"]}]'\n",
      " '[{\"Label\": \"Pittsburgh\", \"Type\": \"G\", \"WikidataId\": \"Q1342\", \"Confidence\": 1.0, \"OccurrenceOffsets\": [0], \"SurfaceForms\": [\"Pittsburgh\"]}, {\"Label\": \"Community development\", \"Type\": \"C\", \"WikidataId\": \"Q718998\", \"Confidence\": 0.987, \"OccurrenceOffsets\": [39], \"SurfaceForms\": [\"Community Development\"]}]'\n",
      " '[{\"Label\": \"Council Bluffs, Iowa\", \"Type\": \"G\", \"WikidataId\": \"Q695565\", \"Confidence\": 1.0, \"OccurrenceOffsets\": [0], \"SurfaceForms\": [\"Council Bluffs\"]}]'\n",
      " '[{\"Label\": \"Boston Bruins\", \"Type\": \"O\", \"WikidataId\": \"Q194121\", \"Confidence\": 0.997, \"OccurrenceOffsets\": [0], \"SurfaceForms\": [\"Bruins\"]}]'\n",
      " '[{\"Label\": \"New England Patriots\", \"Type\": \"O\", \"WikidataId\": \"Q193390\", \"Confidence\": 1.0, \"OccurrenceOffsets\": [22], \"SurfaceForms\": [\"Patriots\"]}, {\"Label\": \"Stephon Gilmore\", \"Type\": \"P\", \"WikidataId\": \"Q3973224\", \"Confidence\": 1.0, \"OccurrenceOffsets\": [0], \"SurfaceForms\": [\"Stephon Gilmore\"]}]']\n"
     ]
    }
   ],
   "source": [
    "# Look at a few non-empty rows\n",
    "sample = news_df[news_df[\"title_entities\"] != \"[]\"].sample(5)\n",
    "print(sample[\"title_entities\"].values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "58b51357",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['Label', 'Type', 'WikidataId', 'Confidence', 'OccurrenceOffsets', 'SurfaceForms'])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ast\n",
    "\n",
    "# Look at just one random non-empty row\n",
    "sample = news_df[news_df[\"title_entities\"] != \"[]\"][\"title_entities\"].sample(1).values[0]\n",
    "\n",
    "# Parse it\n",
    "parsed = ast.literal_eval(sample)\n",
    "\n",
    "# Get all keys from one example entity\n",
    "print(parsed[0].keys())\n",
    "3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cfc509ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title entity types: {'S', 'G', 'Q', 'M', 'U', 'R', 'O', 'C', 'B', 'E', 'V', 'H', 'Y', 'W', 'L', 'J', 'P', 'N', 'I', 'A', 'F', 'K'}\n",
      "Abstract entity types: {'S', 'G', 'Q', 'M', 'U', 'R', 'O', 'C', 'B', 'E', 'V', 'H', 'Y', 'W', 'L', 'J', 'P', 'N', 'I', 'A', 'F', 'K'}\n"
     ]
    }
   ],
   "source": [
    "def extract_all_types(entity_col):\n",
    "    types = set()\n",
    "    for item in entity_col.dropna():\n",
    "        if item.strip() == \"[]\":\n",
    "            continue\n",
    "        try:\n",
    "            parsed = ast.literal_eval(item)\n",
    "            if isinstance(parsed, list):\n",
    "                for ent in parsed:\n",
    "                    if isinstance(ent, dict) and \"Type\" in ent:\n",
    "                        types.add(ent[\"Type\"])\n",
    "        except (ValueError, SyntaxError):\n",
    "            continue\n",
    "    return types\n",
    "\n",
    "# Now run it\n",
    "title_entity_types = extract_all_types(news_df[\"title_entities\"])\n",
    "abstract_entity_types = extract_all_types(news_df[\"abstract_entities\"])\n",
    "\n",
    "print(f\"Title entity types: {title_entity_types}\")\n",
    "print(f\"Abstract entity types: {abstract_entity_types}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ff48745b",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df[\"title\"] = news_df[\"title\"].str.lower()\n",
    "news_df[\"abstract\"] = news_df[\"abstract\"].str.lower()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "921d4640",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def strip_html(text):\n",
    "    return BeautifulSoup(text, \"html.parser\").get_text() if pd.notna(text) else \"\"\n",
    "\n",
    "news_df[\"title\"] = news_df[\"title\"].apply(strip_html)\n",
    "news_df[\"abstract\"] = news_df[\"abstract\"].apply(strip_html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "cfeb8ba9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Cleaned data saved as 'articles_clean.csv'\n"
     ]
    }
   ],
   "source": [
    "# Save cleaned dataset\n",
    "news_df[[\"title\", \"abstract\", \"title_entities\", \"category\"]].to_csv(\"articles_clean.csv\", index=False)\n",
    "\n",
    "print(\"✅ Cleaned data saved as 'articles_clean.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "33596954",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔵 Encoding articles 0 to 10000...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 79/79 [03:02<00:00,  2.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔵 Encoding articles 10000 to 20000...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 79/79 [03:42<00:00,  2.82s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔵 Encoding articles 20000 to 30000...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 79/79 [03:42<00:00,  2.81s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔵 Encoding articles 30000 to 40000...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 79/79 [03:53<00:00,  2.95s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔵 Encoding articles 40000 to 50000...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 79/79 [03:30<00:00,  2.67s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔵 Encoding articles 50000 to 60000...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 79/79 [03:13<00:00,  2.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔵 Encoding articles 60000 to 70000...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 79/79 [03:43<00:00,  2.83s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔵 Encoding articles 70000 to 80000...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 79/79 [03:27<00:00,  2.63s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔵 Encoding articles 80000 to 90000...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 79/79 [03:54<00:00,  2.96s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔵 Encoding articles 90000 to 96106...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 48/48 [02:38<00:00,  3.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ All embeddings generated and checkpoints saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# --- 2. Build Embeddings with Checkpoints ---\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Load cleaned data\n",
    "articles_df = pd.read_csv(\"articles_clean.csv\")\n",
    "\n",
    "# Prepare text for embedding (title + abstract combined)\n",
    "articles_df[\"text_for_embedding\"] = articles_df[\"title\"] + \" \" + articles_df[\"abstract\"]\n",
    "\n",
    "# Load embedding model\n",
    "model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "\n",
    "# Parameters for batching and checkpointing\n",
    "texts = articles_df[\"text_for_embedding\"].tolist()\n",
    "batch_size = 128\n",
    "checkpoint_every = 10000  # Save every 10,000 articles\n",
    "save_dir = \"embedding_checkpoints\"\n",
    "\n",
    "# Make checkpoint directory if not exists\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# Encode and save in chunks\n",
    "n_total = len(texts)\n",
    "all_embeddings = []\n",
    "\n",
    "for start_idx in range(0, n_total, checkpoint_every):\n",
    "    end_idx = min(start_idx + checkpoint_every, n_total)\n",
    "    batch_texts = texts[start_idx:end_idx]\n",
    "    \n",
    "    print(f\"🔵 Encoding articles {start_idx} to {end_idx}...\")\n",
    "\n",
    "    batch_embeddings = model.encode(\n",
    "        batch_texts,\n",
    "        batch_size=batch_size,\n",
    "        convert_to_numpy=True,\n",
    "        show_progress_bar=True\n",
    "    )\n",
    "    \n",
    "    # Save checkpoint immediately\n",
    "    checkpoint_path = os.path.join(save_dir, f\"embeddings_{start_idx}_{end_idx}.npy\")\n",
    "    np.save(checkpoint_path, batch_embeddings)\n",
    "    \n",
    "    # Optionally collect in memory\n",
    "    all_embeddings.append(batch_embeddings)\n",
    "\n",
    "print(\"✅ All embeddings generated and checkpoints saved!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "03c529fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Merged all embeddings: (96106, 384)\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "\n",
    "# Load all saved checkpoints\n",
    "embedding_files = sorted(glob.glob(\"embedding_checkpoints/embeddings_*.npy\"))\n",
    "\n",
    "all_embeddings = []\n",
    "for f in embedding_files:\n",
    "    batch = np.load(f)\n",
    "    all_embeddings.append(batch)\n",
    "\n",
    "# Merge into one big array\n",
    "final_embeddings = np.vstack(all_embeddings)\n",
    "\n",
    "print(\"✅ Merged all embeddings:\", final_embeddings.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "08a8cfc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ FAISS index saved as 'articles_faiss.index' and article metadata saved as 'articles_with_embeddings.csv'\n"
     ]
    }
   ],
   "source": [
    "# --- 3. Create FAISS Index ---\n",
    "\n",
    "import faiss\n",
    "\n",
    "# Prepare embeddings as numpy array\n",
    "embeddings_np = np.array(final_embeddings)\n",
    "\n",
    "# Build FAISS index\n",
    "embedding_dim = embeddings_np.shape[1]  # 384 for MiniLM model\n",
    "index = faiss.IndexFlatL2(embedding_dim)  # L2 = Euclidean distance\n",
    "index.add(embeddings_np)\n",
    "\n",
    "# Save the index\n",
    "faiss.write_index(index, \"articles_faiss.index\")\n",
    "\n",
    "# Save article IDs separately\n",
    "articles_df.to_csv(\"articles_with_embeddings.csv\", index=False)\n",
    "\n",
    "print(\"✅ FAISS index saved as 'articles_faiss.index' and article metadata saved as 'articles_with_embeddings.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "309af765",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🎯 Recommended Articles (with Rewritten Headlines):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>rewritten_title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>49187</th>\n",
       "      <td>this robotic hand taught itself to solve a rub...</td>\n",
       "      <td>The robots are a slick, slick, and slick, but ...</td>\n",
       "      <td>there are a number of very impressive examples...</td>\n",
       "      <td>news</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47326</th>\n",
       "      <td>these researchers are using ai drones to more ...</td>\n",
       "      <td>The research is aimed at a new way to speed up...</td>\n",
       "      <td>researchers are looking to new advances in com...</td>\n",
       "      <td>news</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72594</th>\n",
       "      <td>with face scans, automated marking, singapore ...</td>\n",
       "      <td>The strategy is to create a niche for itself i...</td>\n",
       "      <td>singapore has unveiled an ambitious strategy t...</td>\n",
       "      <td>news</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79083</th>\n",
       "      <td>openai published the tool that writes disturbi...</td>\n",
       "      <td>Openai published the full ai in april</td>\n",
       "      <td>in february, openai announced that it had deve...</td>\n",
       "      <td>news</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86899</th>\n",
       "      <td>david m. shribman: how artificial intelligence...</td>\n",
       "      <td>The sex of the sex of the sex of the sex of th...</td>\n",
       "      <td>montreal   in a classic startup setting   in a...</td>\n",
       "      <td>sports</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   title  \\\n",
       "49187  this robotic hand taught itself to solve a rub...   \n",
       "47326  these researchers are using ai drones to more ...   \n",
       "72594  with face scans, automated marking, singapore ...   \n",
       "79083  openai published the tool that writes disturbi...   \n",
       "86899  david m. shribman: how artificial intelligence...   \n",
       "\n",
       "                                         rewritten_title  \\\n",
       "49187  The robots are a slick, slick, and slick, but ...   \n",
       "47326  The research is aimed at a new way to speed up...   \n",
       "72594  The strategy is to create a niche for itself i...   \n",
       "79083              Openai published the full ai in april   \n",
       "86899  The sex of the sex of the sex of the sex of th...   \n",
       "\n",
       "                                                abstract category  \n",
       "49187  there are a number of very impressive examples...     news  \n",
       "47326  researchers are looking to new advances in com...     news  \n",
       "72594  singapore has unveiled an ambitious strategy t...     news  \n",
       "79083  in february, openai announced that it had deve...     news  \n",
       "86899  montreal   in a classic startup setting   in a...   sports  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --- 4. Recommend Articles ---\n",
    "\n",
    "import pandas as pd\n",
    "import faiss\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import pipeline\n",
    "\n",
    "# Load index and articles\n",
    "index = faiss.read_index(\"articles_faiss.index\")\n",
    "articles_df = pd.read_csv(\"articles_with_embeddings.csv\")\n",
    "\n",
    "# Load embedding model\n",
    "model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "\n",
    "# Load lightweight LLM for rewriting\n",
    "rewrite_pipeline = pipeline(\"text2text-generation\", model=\"google/flan-t5-small\")\n",
    "\n",
    "# Define a sample editorial query\n",
    "editorial_query = \"artificial intelligence innovation\"\n",
    "\n",
    "# Embed the query\n",
    "query_embedding = model.encode([editorial_query])\n",
    "\n",
    "# Search top 5 articles\n",
    "D, I = index.search(np.array(query_embedding), k=5)  # D = distance, I = index\n",
    "\n",
    "# Get recommended articles as a DataFrame\n",
    "recommended_articles = articles_df.iloc[I[0]].copy()  # <-- copy to avoid SettingWithCopyWarning\n",
    "\n",
    "# Define rewriting function\n",
    "def rewrite_headline(title, abstract):\n",
    "    prompt = f\"Rewrite the news headline to be more engaging and SEO-friendly:\\n\\nTitle: {title}\\n\\nAbstract: {abstract}\\n\\nRewritten Headline:\"\n",
    "    response = rewrite_pipeline(prompt, max_length=30, do_sample=False)\n",
    "    return response[0]['generated_text']\n",
    "\n",
    "# Apply headline rewriting\n",
    "recommended_articles[\"rewritten_title\"] = recommended_articles.apply(\n",
    "    lambda row: rewrite_headline(row[\"title\"], row[\"abstract\"]), axis=1\n",
    ")\n",
    "\n",
    "# Display the rewritten articles\n",
    "print(\"\\n Recommended Articles (with Rewritten Headlines):\")\n",
    "display(recommended_articles[[\"title\", \"rewritten_title\", \"abstract\", \"category\"]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de602b50",
   "metadata": {},
   "source": [
    "### Load FAISS index, articles metadata, and embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "69cb70d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded FAISS index, articles, and MiniLM model.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import faiss\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Load FAISS index\n",
    "index = faiss.read_index(\"articles_faiss.index\")\n",
    "\n",
    "# Load articles metadata\n",
    "articles_df = pd.read_csv(\"articles_with_embeddings.csv\")\n",
    "\n",
    "# Load embedding model\n",
    "model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "\n",
    "print(\"✅ Loaded FAISS index, articles, and MiniLM model.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "443ad319",
   "metadata": {},
   "source": [
    "### Define editorial queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "58aa92c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define editorial topics (\"editorial queries\")\n",
    "editorial_queries = {\n",
    "    \"Top Technology News\": \"latest breakthroughs in technology and innovation\",\n",
    "    \"Inspiring Stories\": \"positive and uplifting news stories\",\n",
    "    \"Global Politics\": \"latest news about world politics and diplomacy\",\n",
    "    \"Climate and Environment\": \"climate change news and environment protection\",\n",
    "    \"Health and Wellness\": \"advances in healthcare and medical discoveries\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00678506",
   "metadata": {},
   "source": [
    "### Run retrieval for each topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6284b0f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Retrieved top articles for each editorial topic.'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "retrieved_articles = {}\n",
    "\n",
    "for topic, articles in retrieved_articles.items():\n",
    "    articles[\"rewritten_title\"] = articles.apply(\n",
    "        lambda row: rewrite_headline(row[\"title\"], row[\"abstract\"]), axis=1\n",
    "    )\n",
    "\n",
    "display(\"Retrieved top articles for each editorial topic.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43deb84b",
   "metadata": {},
   "source": [
    "### LLM Headline Rewriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960b11ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "for topic, articles in retrieved_articles.items():\n",
    "    articles[\"rewritten_title\"] = articles.apply(\n",
    "        lambda row: rewrite_headline(row[\"title\"], row[\"abstract\"]), axis=1\n",
    "    )\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ff5195",
   "metadata": {},
   "source": [
    "### LLM Editorial Explainer Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a419c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the explanation generation function\n",
    "def generate_explanation(title, abstract):\n",
    "    prompt = f\"Explain in one sentence why this news article is important to readers:\\n\\nTitle: {title}\\n\\nAbstract: {abstract}\\n\\nExplanation:\"\n",
    "    response = rewrite_pipeline(prompt, max_length=40, do_sample=False)\n",
    "    return response[0]['generated_text']\n",
    "\n",
    "# Apply explanation generation to each topic's articles\n",
    "for topic, articles in retrieved_articles.items():\n",
    "    articles[\"explanation\"] = articles.apply(\n",
    "        lambda row: generate_explanation(row[\"title\"], row[\"abstract\"]), axis=1\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddeac985",
   "metadata": {},
   "source": [
    "### Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "6d2a5228",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Today's fresh topics: []\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def save_topics(topics, filename=\"memory_topics.json\"):\n",
    "    with open(filename, \"w\") as f:\n",
    "        json.dump(topics, f)\n",
    "\n",
    "def load_topics(filename=\"memory_topics.json\"):\n",
    "    try:\n",
    "        with open(filename, \"r\") as f:\n",
    "            return json.load(f)\n",
    "    except FileNotFoundError:\n",
    "        return []\n",
    "\n",
    "# Example usage:\n",
    "yesterday_topics = load_topics()\n",
    "\n",
    "# Compare today's topics\n",
    "fresh_topics = [t for t in editorial_queries if t not in yesterday_topics]\n",
    "\n",
    "# Use fresh topics for retrieval\n",
    "print(f\"✅ Today's fresh topics: {fresh_topics}\")\n",
    "\n",
    "# After today's run\n",
    "save_topics(editorial_queries)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98a97ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from glob import glob\n",
    "\n",
    "# Find all files starting with 'retrieved_'\n",
    "csv_files = glob(\"retrieved_*.csv\")\n",
    "\n",
    "# Combine all retrieved topics into one DataFrame\n",
    "dfs = [pd.read_csv(file).assign(topic=file.replace(\"retrieved_\", \"\").replace(\".csv\", \"\").replace(\"_\", \" \").title()) for file in csv_files]\n",
    "combined_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "display(combined_df[[\"topic\", \"title\", \"rewritten_title\", \"explanation\"]].head(20))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
