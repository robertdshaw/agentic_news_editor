{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232b06d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rshaw\\Desktop\\EC Utbildning - Data Science\\Thesis\\Agentic_AI_News_Editor project\\agentic_ai_editor_project\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rshaw\\Desktop\\EC Utbildning - Data Science\\Thesis\\Agentic_AI_News_Editor project\\agentic_ai_editor_project\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\rshaw\\.cache\\huggingface\\hub\\models--distilbert-base-uncased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "2025-05-07 22:17:29,809 - WARNING - Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded DistilBERT model successfully\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import logging\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import re\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Initialize device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load models\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "    bert_model = AutoModel.from_pretrained(\"distilbert-base-uncased\")\n",
    "    bert_model = bert_model.to(device)\n",
    "    print(\"Loaded DistilBERT model successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"Failed to load DistilBERT model: {e}\")\n",
    "    \n",
    "test_input = tokenizer(\"Test headline\", return_tensors=\"pt\").to(device)\n",
    "with torch.no_grad():\n",
    "    output = bert_model(**test_input)\n",
    "print(\"Model test passed. Output shape:\", output.last_hidden_state.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef81ec7f",
   "metadata": {},
   "source": [
    "## Load Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "846fee73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 95492 headlines with CTR data\n",
      "\n",
      "Data preview:\n",
      "   newsID   category               subcategory  \\\n",
      "0  N88753  lifestyle           lifestyleroyals   \n",
      "1  N45436       news  newsscienceandtechnology   \n",
      "2  N23144     health                weightloss   \n",
      "3  N93187       news                 newsworld   \n",
      "4  N75236     health                    voices   \n",
      "\n",
      "                                               title  \\\n",
      "0  The Brands Queen Elizabeth, Prince Charles, an...   \n",
      "1    Walmart Slashes Prices on Last-Generation iPads   \n",
      "2                      50 Worst Habits For Belly Fat   \n",
      "3  The Cost of Trump's Aid Freeze in the Trenches...   \n",
      "4  I Was An NBA Wife. Here's How It Affected My M...   \n",
      "\n",
      "                                            abstract  \\\n",
      "0  Shop the notebooks, jackets, and more that the...   \n",
      "1  Apple's new iPad releases bring big deals on l...   \n",
      "2  These seemingly harmless habits are holding yo...   \n",
      "3  Lt. Ivan Molchanets peeked over a parapet of s...   \n",
      "4  I felt like I was a fraud, and being an NBA wi...   \n",
      "\n",
      "                                             url  \\\n",
      "0  https://assets.msn.com/labs/mind/AAGH0ET.html   \n",
      "1  https://assets.msn.com/labs/mind/AABmf2I.html   \n",
      "2  https://assets.msn.com/labs/mind/AAB19MK.html   \n",
      "3  https://assets.msn.com/labs/mind/AAJgNsz.html   \n",
      "4  https://assets.msn.com/labs/mind/AACk2N6.html   \n",
      "\n",
      "                                      title_entities  \\\n",
      "0  [{\"Label\": \"Prince Philip, Duke of Edinburgh\",...   \n",
      "1  [{\"Label\": \"IPad\", \"Type\": \"J\", \"WikidataId\": ...   \n",
      "2  [{\"Label\": \"Adipose tissue\", \"Type\": \"C\", \"Wik...   \n",
      "3                                                 []   \n",
      "4                                                 []   \n",
      "\n",
      "                                   abstract_entities  title_length  \\\n",
      "0                                                 []            70   \n",
      "1  [{\"Label\": \"IPad\", \"Type\": \"J\", \"WikidataId\": ...            47   \n",
      "2  [{\"Label\": \"Adipose tissue\", \"Type\": \"C\", \"Wik...            29   \n",
      "3  [{\"Label\": \"Ukraine\", \"Type\": \"G\", \"WikidataId...            63   \n",
      "4  [{\"Label\": \"National Basketball Association\", ...            59   \n",
      "\n",
      "   abstract_length  title_reading_ease news_id  total_clicks  \\\n",
      "0               73               77.23     NaN           0.0   \n",
      "1               64                6.17     NaN           0.0   \n",
      "2              116               90.77     NaN           0.0   \n",
      "3              196              101.60     NaN           0.0   \n",
      "4               99               82.31     NaN           0.0   \n",
      "\n",
      "   total_impressions  ctr  \n",
      "0                0.0  0.0  \n",
      "1                0.0  0.0  \n",
      "2                0.0  0.0  \n",
      "3                0.0  0.0  \n",
      "4                0.0  0.0  \n",
      "\n",
      "Missing values:\n",
      "title    0\n",
      "ctr      0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Define data directory\n",
    "processed_data_dir = 'agentic_news_editor/processed_data'\n",
    "\n",
    "def load_training_data(data_dir=processed_data_dir):\n",
    "    \"\"\"Load processed headlines with CTR data\"\"\"\n",
    "    try:\n",
    "        headlines_path = os.path.join(data_dir, 'news_with_engagement.csv')\n",
    "        if not os.path.exists(headlines_path):\n",
    "            print(f\"Training data not found at {headlines_path}\")\n",
    "            return None\n",
    "                \n",
    "        headline_data = pd.read_csv(headlines_path)\n",
    "        print(f\"Loaded {len(headline_data)} headlines with CTR data\")\n",
    "        \n",
    "        # Preview the data\n",
    "        print(\"\\nData preview:\")\n",
    "        print(headline_data.head())\n",
    "        \n",
    "        # Check for missing values\n",
    "        print(\"\\nMissing values:\")\n",
    "        print(headline_data[['title', 'ctr']].isna().sum())\n",
    "        \n",
    "        return headline_data\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading training data: {e}\")\n",
    "        return None\n",
    "\n",
    "# Load the data\n",
    "display(headline_data = load_training_data())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ee4f26",
   "metadata": {},
   "source": [
    "## Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1cf6a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(headlines, tokenizer=tokenizer, bert_model=bert_model, device=device):\n",
    "    \"\"\"Extract features from headlines for model training\"\"\"\n",
    "    print(f\"Extracting features from {len(headlines)} headlines\")\n",
    "    features_list = []\n",
    "    \n",
    "    # Process in smaller batches to avoid memory issues\n",
    "    batch_size = 20  # Smaller for notebook testing\n",
    "    \n",
    "    # Sample a few headlines for testing in notebook\n",
    "    if len(headlines) > 100:\n",
    "        test_headlines = headlines[:100]\n",
    "        print(f\"Using first 100 headlines for testing\")\n",
    "    else:\n",
    "        test_headlines = headlines\n",
    "    \n",
    "    for i in range(0, len(test_headlines), batch_size):\n",
    "        batch = test_headlines[i:i+batch_size]\n",
    "        print(f\"Processing batch {i//batch_size + 1}/{(len(test_headlines)-1)//batch_size + 1}\")\n",
    "        \n",
    "        for headline in batch:\n",
    "            features = {}\n",
    "            \n",
    "            # Basic features based on EDA findings\n",
    "            features['length'] = len(headline)\n",
    "            features['word_count'] = len(headline.split())\n",
    "            features['has_number'] = int(bool(re.search(r'\\d', headline)))\n",
    "            features['num_count'] = len(re.findall(r'\\d+', headline))\n",
    "            features['is_question'] = int(headline.endswith('?') or \n",
    "                                       headline.lower().startswith(('how', 'what', 'why', 'where', 'when', 'is ')))\n",
    "            features['has_colon'] = int(':' in headline)\n",
    "            features['has_quote'] = int('\"' in headline or \"'\" in headline)\n",
    "            features['has_how_to'] = int('how to' in headline.lower())\n",
    "            \n",
    "            # Get embedding for semantic features\n",
    "            try:\n",
    "                inputs = tokenizer(headline, return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n",
    "                inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    outputs = bert_model(**inputs)\n",
    "                \n",
    "                # Use the [CLS] token embedding\n",
    "                embedding = outputs.last_hidden_state[:, 0, :].cpu().numpy()[0]\n",
    "                \n",
    "                # Add first 10 embedding dimensions as features\n",
    "                for j in range(10):\n",
    "                    features[f'emb_{j}'] = embedding[j]\n",
    "            except Exception as e:\n",
    "                print(f\"Error extracting embedding for '{headline}': {e}\")\n",
    "                # Add zero embeddings if failed\n",
    "                for j in range(10):\n",
    "                    features[f'emb_{j}'] = 0.0\n",
    "            \n",
    "            features_list.append(features)\n",
    "    \n",
    "    features_df = pd.DataFrame(features_list)\n",
    "    \n",
    "    # Display feature statistics\n",
    "    print(\"\\nFeature statistics:\")\n",
    "    print(features_df.describe())\n",
    "    \n",
    "    return features_df\n",
    "\n",
    "# Clean data and extract features\n",
    "if headline_data is not None:\n",
    "    # Handle NaN values\n",
    "    clean_headline_data = headline_data.dropna(subset=['title', 'ctr'])\n",
    "    print(f\"Clean data shape: {clean_headline_data.shape}\")\n",
    "    \n",
    "    # Extract features for a sample\n",
    "    features_df = extract_features(clean_headline_data['title'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d7733c",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9973765",
   "metadata": {},
   "source": [
    "def train_model(features_df, ctr_values, output_file='headline_ctr_model.pkl'):\n",
    "    \"\"\"Train a model to predict headline CTR\"\"\"\n",
    "    print(\"Training headline CTR prediction model\")\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        features_df, ctr_values, test_size=0.2, random_state=42\n",
    "    )\n",
    "    \n",
    "    print(f\"Training set: {X_train.shape[0]} samples\")\n",
    "    print(f\"Test set: {X_test.shape[0]} samples\")\n",
    "    \n",
    "    # Define and train model\n",
    "    model = RandomForestRegressor(\n",
    "        n_estimators=100, \n",
    "        max_depth=10,\n",
    "        min_samples_split=10,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Evaluate model\n",
    "    y_pred = model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    print(f\"Model evaluation - MSE: {mse:.4f}, R²: {r2:.4f}\")\n",
    "    \n",
    "    # Feature importance\n",
    "    feature_importances = pd.DataFrame({\n",
    "        'feature': features_df.columns,\n",
    "        'importance': model.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    print(\"\\nTop 10 important features:\")\n",
    "    for i, row in feature_importances.head(10).iterrows():\n",
    "        print(f\"  {row['feature']}: {row['importance']:.4f}\")\n",
    "    \n",
    "    # Plot feature importance\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(x='importance', y='feature', data=feature_importances.head(10))\n",
    "    plt.title('Top 10 Feature Importances for CTR Prediction')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Save model - comment this out during testing if desired\n",
    "    # with open(output_file, 'wb') as f:\n",
    "    #     pickle.dump(model, f)\n",
    "    # print(f\"Model saved to {output_file}\")\n",
    "    \n",
    "    return {\n",
    "        'model': model,\n",
    "        'mse': mse,\n",
    "        'r2': r2,\n",
    "        'feature_importances': feature_importances\n",
    "    }\n",
    "\n",
    "# Train model if features are available\n",
    "if 'features_df' in locals() and len(features_df) > 0:\n",
    "    result = train_model(features_df, clean_headline_data['ctr'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34fedd89",
   "metadata": {},
   "source": [
    "## Model Report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da2b4609",
   "metadata": {},
   "source": [
    "def create_model_report(result, headline_data, save_to_file=False):\n",
    "    \"\"\"Create a markdown report about the model performance\"\"\"\n",
    "    if result is None:\n",
    "        print(\"No model result to report\")\n",
    "        return\n",
    "    \n",
    "    report = f\"\"\"# Headline CTR Prediction Model Report\n",
    "Generated: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M')}\n",
    "\n",
    "## Model Performance\n",
    "- Mean Squared Error: {result['mse']:.4f}\n",
    "- R-squared: {result['r2']:.4f}\n",
    "\n",
    "## Dataset Summary\n",
    "- Total headlines analyzed: {len(headline_data)}\n",
    "- CTR range: {headline_data['ctr'].min():.2f} to {headline_data['ctr'].max():.2f}\n",
    "- Mean CTR: {headline_data['ctr'].mean():.2f}\n",
    "\n",
    "## Key Feature Importances\n",
    "\"\"\"\n",
    "    \n",
    "    for i, row in result['feature_importances'].head(10).iterrows():\n",
    "        report += f\"- {row['feature']}: {row['importance']:.4f}\\n\"\n",
    "    \n",
    "    report += \"\"\"\n",
    "## Usage Guidelines\n",
    "This model can be used to predict the expected CTR of news headlines.\n",
    "It's integrated with the HeadlineMetrics class for headline evaluation\n",
    "and the HeadlineLearningLoop for continuous improvement.\n",
    "\n",
    "## Features Based on EDA\n",
    "The model uses features derived from EDA findings:\n",
    "- Questions in headlines significantly reduce CTR\n",
    "- Numbers in headlines can reduce CTR if used inappropriately\n",
    "- Headline length and structure matter for engagement\n",
    "- Category-specific patterns influence performance\n",
    "\"\"\"\n",
    "    \n",
    "    # Print report for notebook review\n",
    "    print(report)\n",
    "    \n",
    "    # Save to file if requested\n",
    "    if save_to_file:\n",
    "        with open('headline_model_report.md', 'w') as f:\n",
    "            f.write(report)\n",
    "        print(\"Model report saved to headline_model_report.md\")\n",
    "\n",
    "# Create report if results are available\n",
    "if 'result' in locals() and result is not None:\n",
    "    create_model_report(result, clean_headline_data, save_to_file=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3435206",
   "metadata": {},
   "source": [
    "## Full Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f3bcb05",
   "metadata": {},
   "source": [
    "def run_training_pipeline(save_model=False):\n",
    "    \"\"\"Run the complete model training pipeline\"\"\"\n",
    "    # Load data\n",
    "    headline_data = load_training_data()\n",
    "    if headline_data is None:\n",
    "        print(\"Could not load training data. Aborting.\")\n",
    "        return None\n",
    "    \n",
    "    # Handle NaN values\n",
    "    clean_data = headline_data.dropna(subset=['title', 'ctr'])\n",
    "    \n",
    "    # Extract features\n",
    "    features_df = extract_features(clean_data['title'].values)\n",
    "    \n",
    "    # Train model\n",
    "    result = train_model(features_df, clean_data['ctr'].values)\n",
    "    \n",
    "    # Create a report\n",
    "    create_model_report(result, headline_data, save_to_file=save_model)\n",
    "    \n",
    "    if save_model:\n",
    "        # Save model\n",
    "        with open('headline_ctr_model.pkl', 'wb') as f:\n",
    "            pickle.dump(result['model'], f)\n",
    "        print(\"Model saved to headline_ctr_model.pkl\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Uncommment to run full pipeline\n",
    "# final_result = run_training_pipeline(save_model=True)\n",
    "# if final_result is not None:\n",
    "#     print(f\"Model training complete. R-squared: {final_result['r2']:.4f}\")\n",
    "# else:\n",
    "#     print(\"Model training failed.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
